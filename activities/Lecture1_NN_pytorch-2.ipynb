{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "idc7tv4H8zs8",
      "metadata": {
        "id": "idc7tv4H8zs8"
      },
      "source": [
        "# Neural Networks in `PyTorch`\n",
        "\n",
        "\n",
        "\n",
        "We will use a dense neural network in `torch` to solve a simple regression problem using physics data.\n",
        "\n",
        "## Learning Task\n",
        "We will construct a dense neural network to predict the invariant mass of a particle from its energy, momentum, (and charge).\n",
        "\n",
        "*Note that this task does not require machine learning. We choose a task with a known mapping to help us create, debug, and tune our first neural network.*\n",
        "\n",
        "## Dataset\n",
        "This dataset is a collection of simulated particle events from [Pythia](https://pythia.org/). The dataset is a 2D array where each row represents one event from an $e^{-} + p$ collision. This dataset is comprised _only_ of events where exactly 16 particles are produced from an electron-proton collision. Each particle contains $(p_x,p_y,p_z,E,q)$ information. Each event is therefore represented by 80 numbers.\n",
        "\n",
        "**Advanced activity:** There are more interesting event-wise learning tasks using this dataset. Consider crafting your own learning task and target for this data.\n",
        "\n",
        "\n",
        "\n",
        "## Computational Notes\n",
        "\n",
        "If this is your first time in a Jupyter-like environment, please read the following carefully:\n",
        "\n",
        " - You are in an active kernel\n",
        " - Run each cell with `Shift + Enter`\n",
        " - You must execute the cells in the order that you want the code to run\n",
        " - `Runtime`$\\rightarrow$`Change runtime type` allows you to utilize GPUs and TPUs. They are unnecessary here, but will become vital in later exercises.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1al9mIE8zs-",
      "metadata": {
        "id": "d1al9mIE8zs-"
      },
      "outputs": [],
      "source": [
        "# Imports (PyTorch instead of TensorFlow)\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d_J2FTqB8zs-",
      "metadata": {
        "id": "d_J2FTqB8zs-"
      },
      "source": [
        "## Data loading\n",
        "\n",
        "If you already have the data file (e.g. `particle-events.npy`) in your working directory, you can skip this step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9zgO4bil8zs_",
      "metadata": {
        "id": "9zgO4bil8zs_"
      },
      "outputs": [],
      "source": [
        "# import data from github. Note: in colab, go to Files and refresh to see file\n",
        "# here I use Linux commands within the notebook to pull the data file and rename it\n",
        "!wget https://github.com/NuclearTalent/MachineLearningECT/blob/master/doc/pub/Day6/data/homogenous-16-particle-events-energy.npy?raw=true\n",
        "!mv homogenous-16-particle-events-energy.npy?raw=true particle-events.npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sS1RAUXQ8zs_",
      "metadata": {
        "id": "sS1RAUXQ8zs_"
      },
      "outputs": [],
      "source": [
        "# now we load the data file, which is a numpy array\n",
        "events = np.load(\"particle-events.npy\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NpVbPnVG8zs_",
      "metadata": {
        "id": "NpVbPnVG8zs_"
      },
      "source": [
        "Recall that each row of this dataset is an entire event. We need each row to represent a training example, which is a single particle.\n",
        "\n",
        "Using `numpy`'s `reshape` method we can make each row represent one particle.\n",
        "\n",
        "At first, we will only use $(p_x,p_y,p_z,E)$ as our features for training a model. We will exclude the charge information. This should be fine, since charge is not needed to compute our target, invariant mass.\n",
        "\n",
        "We will only use 100 training examples for our first training.\n",
        "\n",
        "Create a numpy array for this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fPDbdEz48zs_",
      "metadata": {
        "id": "fPDbdEz48zs_"
      },
      "outputs": [],
      "source": [
        "# Here we rearrange the data within each of the events to isolate particles\n",
        "\n",
        "n_part_per_event = 16      # number of particles per event\n",
        "n_feat_per_part  = 5       # features per particle (example). If your in\n",
        "\n",
        "evt_particles = events.reshape( len(events), n_part_per_event, n_feat_per_part)\n",
        "\n",
        "#print(\"evt_particles[0] =\\n\", evt_particles[0])\n",
        "\n",
        "# Use another call of reshape to combine all events to have the appropriate shape\n",
        "# Complete me:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create a smaller subdataset\n",
        "\n",
        "# complete me\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "errL03R68ztA",
      "metadata": {
        "id": "errL03R68ztA"
      },
      "source": [
        "These are our training data inputs, but we also must provide the targets, which are the invariant masses of each particle. This is a straightforward computation.\n",
        "\n",
        "We choose units where $c = 1$:\n",
        "$$m^2=E^2-||\\textbf{p}||^2$$\n",
        "where $m, E$, and $\\textbf{p}$ are all in GeV.\n",
        "\n",
        "**Create an array of your target values, the invariant masses (*not* $m^2$).**\n",
        "Due to insufficient precision, some $m^2$ values for massless particles will come out very slightly negative. These should be treated as zero to avoid `NaN`. I used the `maximum` method from `numpy` to handle this.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XKBV40fxULpS",
      "metadata": {
        "id": "XKBV40fxULpS"
      },
      "outputs": [],
      "source": [
        "# Complete me:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H9SIt6_mUSOh",
      "metadata": {
        "id": "H9SIt6_mUSOh"
      },
      "source": [
        "Next, make a histogram of the target data to make sure that we are seeing masses of real particles. As this data has limited precision, this will not resolve electrons very well, but protons, pions, and massless particles should be clearly visible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XTLEFW7V8ztA",
      "metadata": {
        "id": "XTLEFW7V8ztA"
      },
      "outputs": [],
      "source": [
        "plt.hist(m, bins=100)\n",
        "plt.xlabel(\"mass (GeV)\")\n",
        "plt.ylabel(\"count\")\n",
        "plt.title(\"target distribution\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SGDXbymO8ztA",
      "metadata": {
        "id": "SGDXbymO8ztA"
      },
      "source": [
        "## Train/validation split and DataLoaders\n",
        "\n",
        "We will now do a bit more processing to prepare our data for our neural network models. We build our training, validation, and test datasets.\n",
        "\n",
        "We will first split 20\\% of the data for testing, then split 20\\% of the remaining data for validation. These are typical splits, but ideal splitting may vary depending on the amount of data you have and presence of outliers.\n",
        "\n",
        "You may want to use `scikit-learn`'s [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) method for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w52YMPgJ8ztA",
      "metadata": {
        "id": "w52YMPgJ8ztA"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Convert to torch tensors\n",
        "X_tensor = torch.from_numpy(X).float()\n",
        "y_tensor = torch.from_numpy(m).float().unsqueeze(1)  # make y shape (N,1)\n",
        "\n",
        "# complete me\n",
        "\n",
        "\n",
        "\n",
        "# build DataLoader objects\n",
        "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I wrote a simple function for plotting loss curves, which will help us tune our model."
      ],
      "metadata": {
        "id": "ydMg83T3NCrS"
      },
      "id": "ydMg83T3NCrS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JU_1hGfj-RGW",
      "metadata": {
        "id": "JU_1hGfj-RGW"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_learning_curve(history):\n",
        "    plt.plot(history[\"loss\"], label=\"training loss\")\n",
        "    plt.plot(history[\"val_loss\"], label=\"validation loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.yscale(\"log\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ieqyqSa8ztA",
      "metadata": {
        "id": "6ieqyqSa8ztA"
      },
      "source": [
        "## Now we build our first model architecture!\n",
        "\n",
        "We will begin with one hidden layer with 2 nodes that have a ReLU activation function. This is **not** an ideal architecture.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gFHZKQ6Y8ztA",
      "metadata": {
        "id": "gFHZKQ6Y8ztA"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  # Call the parent constructor so nn.Module sets up correctly\n",
        "  def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        # nn.Sequential lets us stack layers/modules in order\n",
        "        # Each layer takes the output of the previous one as input\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 5),\n",
        "            nn.Sigmoid(),\n",
        "            # complete me\n",
        "\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "     # Defines the forward pass: how input x flows through layers\n",
        "     return self.net(x)\n",
        "\n",
        "model = MLP(input_dim).to(device)\n",
        "print(model)\n",
        "\n",
        "# Calculate the number of trainable parameters (check against your own calculation!)\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(trainable_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "haWB2DYn8ztA",
      "metadata": {
        "id": "haWB2DYn8ztA"
      },
      "source": [
        "Defining optimizer, loss, and training loop\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "evPmcWfI8ztA",
      "metadata": {
        "id": "evPmcWfI8ztA"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # try stochastic gradient descent\n",
        "loss_f = nn.MSELoss() # MSE loss function\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, loss_f):\n",
        "    # Put the model in training mode\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Loop over all mini-batches from the DataLoader\n",
        "    for xb, yb in loader:\n",
        "        # Move both inputs (xb) and targets (yb) onto the same device (CPU or GPU)\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        # Reset gradients from the previous step\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass: compute predictions for this batch\n",
        "        preds = model(xb)\n",
        "        # Compute the loss comparing predictions vs. true labels\n",
        "        loss = loss_f(preds, yb)\n",
        "         # Backward pass: compute gradients of all model parameters w.r.t. loss\n",
        "        loss.backward()\n",
        "        # Take one optimization step (update model weights using gradients)\n",
        "        optimizer.step()\n",
        "        # Accumulate the loss, scaled by batch size (for averaging later)\n",
        "        running_loss += loss.item() * xb.size(0)\n",
        "    # Return the average loss across the whole dataset\n",
        "    return running_loss / len(loader.dataset)\n",
        "\n",
        "\n",
        "# the following is for evaluatinf the model, e.g. for validation\n",
        "# compare this with the training loop above and try to understand the differences\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, loss_f):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        preds = model(xb)\n",
        "        loss = loss_f(preds, yb)\n",
        "        running_loss += loss.item() * xb.size(0)\n",
        "    return running_loss / len(loader.dataset)\n",
        "\n",
        "history = {\"loss\": [], \"val_loss\": []}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training loop!\n",
        "epochs = 10  # start with 10\n",
        "for epoch in range(1, epochs+1):\n",
        "    train_loss = # train one epoch\n",
        "    val_loss   = # evalaute on val data\n",
        "    history[\"loss\"].append(train_loss) # keep track of loss\n",
        "    history[\"val_loss\"].append(val_loss) # keep track of val loss\n",
        "    if epoch % 1 == 0: # printing output. You likely want to mod this with higher num epochs\n",
        "        print(f\"Epoch {epoch:02d} | train_loss={train_loss:.6f} | val_loss={val_loss:.6f}\")\n"
      ],
      "metadata": {
        "id": "3WwydxWmQCr-"
      },
      "id": "3WwydxWmQCr-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We now evaluate the model\n",
        "\n",
        "Let's plot our loss curve and make predictions on the test set.\n"
      ],
      "metadata": {
        "id": "m90_0FQ6Qg8P"
      },
      "id": "m90_0FQ6Qg8P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZuJpH_lOHX_H",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZuJpH_lOHX_H"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "all_preds = []\n",
        "true = []\n",
        "with torch.no_grad():\n",
        "    for xt, yt in test_loader:\n",
        "        xt = xt.to(device)\n",
        "        preds = # complete me\n",
        "        all_preds.append(preds.cpu())   # move back to CPU for numpy/plotting\n",
        "        true.append(yt.numpy())\n",
        "\n",
        "\n",
        "# plot loss curve\n",
        "plot_learning_curve(history)\n",
        "# visualize predictions on validation data\n",
        "val_preds = torch.cat(all_preds).numpy()\n",
        "true = np.concatenate(true)\n",
        "nbins = 50\n",
        "plt.hist(true,bins=70,alpha=0.5)\n",
        "plt.hist(val_preds,bins=70)\n",
        "plt.show()\n",
        "print(len(true))\n",
        "print(len(val_preds))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C1DbyXrI8ztA",
      "metadata": {
        "id": "C1DbyXrI8ztA"
      },
      "source": [
        "## Tuning\n",
        "\n",
        "I did *not* start you out with ideal hyperparemeters and architecture. You should now tune your model to be as good as possible, based on information from your loss curve.\n",
        "\n",
        "Some tuning to consider to get the best possible model:\n",
        "\n",
        "1. Would scaling your data help your model?\n",
        "2. Might including charge help your model? Why/why not?\n",
        "2. Might more training data help?\n",
        "2. Was ReLU the best activation function for this network? Other options for a hidden layer include `Sigmoid`, `tanh`, or `ReLU` variants.\n",
        "3. Perhaps we did not have enough model parameters to accurately represent the mapping. You can try to increase the number of nodes in the hidden layer and/or add mor hidden layers\n",
        "3.  Another hyperparameter to adjust is batch size, which is the number of training examples used to calculate the gradient on each step. While you may initially think that a higher batch size leads to faster or more accurate training, in practice this is not true. The \"noise\" that arises from using less training examples at each iteration can actually help find the global minimum of the loss function. (See here for more info: https://arxiv.org/pdf/1609.04836.pdf)\n",
        "4. Another hyperparameter to tune is the *learning rate*.\n",
        "\n",
        " - If the learning rate is too high, we are taking too large of a step in the gradient descent at each iteration and will miss narrow minima in the loss function.\n",
        " - If the learning rate is too small, then we are not traveling far enough in each iteration and we will take far too long to reach a minimum.\n",
        "\n",
        "  Perhaps the learning rate is too high and the network can't fine tune.\n",
        "5. If you see evidence of *overfitting*, meaning the validation loss begins to climb as the training loss decreases, you can try adding [dropout layers](https://docs.pytorch.org/docs/stable/generated/torch.nn.Dropout.html) or [batch normalization](https://docs.pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html).\n",
        "6. There are a suite of [gradient descent-based optimizers](https://docs.pytorch.org/docs/stable/optim.html) to try\n",
        "\n",
        "\n",
        "\n",
        "Even after tuning, you should find (expecially if you predict predictions vs ground truth), that your model isn't great. This is for more fundamental reasons on the learning problem that we set up. *What are they?*"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vVMXofYIUHJW"
      },
      "id": "vVMXofYIUHJW",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}